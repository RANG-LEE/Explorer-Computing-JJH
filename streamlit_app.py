import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import pydeck as pdk
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import os
from matplotlib import rc, font_manager
import platform
import chromedriver_autoinstaller

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
import os
import chromedriver_autoinstaller

@st.cache_resource
def get_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    
    # [ì¤‘ìš”] Streamlit Cloud í™˜ê²½(ë¦¬ëˆ…ìŠ¤) ê²½ë¡œ ê°•ì œ ì§€ì •
    # packages.txtë¡œ ì„¤ì¹˜ëœ chromiumì€ ë³´í†µ ì´ ê²½ë¡œì— ìˆìŠµë‹ˆë‹¤.
    if os.path.exists("/usr/bin/chromium") and os.path.exists("/usr/bin/chromedriver"):
        options.binary_location = "/usr/bin/chromium"
        service = Service("/usr/bin/chromedriver")
        driver = webdriver.Chrome(service=service, options=options)
        return driver
        
    # [ë¡œì»¬ í™˜ê²½] ë‚´ ì»´í“¨í„°(ìœˆë„ìš°/ë§¥)ì—ì„œëŠ” ìë™ ì„¤ì¹˜ ì‚¬ìš©
    try:
        chromedriver_autoinstaller.install()
        driver = webdriver.Chrome(options=options)
        return driver
    except Exception as e:
        st.error(f"ë“œë¼ì´ë²„ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return None


# ==========================================
# [ì„¤ì •] í˜ì´ì§€ ë° í°íŠ¸ ì„¤ì •
# ==========================================
st.set_page_config(
    page_title="ìœµí•© ì¸ì¬ í¬íŠ¸í´ë¦¬ì˜¤",
    page_icon="ğŸ“",
    layout="wide"
)

# í•œê¸€ í°íŠ¸ ì„¤ì • (OSë³„ ìë™ ëŒ€ì‘)
system_name = platform.system()
font_path = None

if system_name == 'Windows':
    font_path = "C:/Windows/Fonts/malgun.ttf"
    try:
        if os.path.exists(font_path):
            font_name = font_manager.FontProperties(fname=font_path).get_name()
            rc('font', family=font_name)
    except:
        pass
elif system_name == 'Darwin': 
    rc('font', family='AppleGothic')
    font_path = '/System/Library/Fonts/Supplemental/AppleGothic.ttf'
else:
    # ë¦¬ëˆ…ìŠ¤/í´ë¼ìš°ë“œ í™˜ê²½ (í•œê¸€ í°íŠ¸ê°€ ì—†ì„ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©)
    pass

plt.rcParams['axes.unicode_minus'] = False

# =========================================================
# 0. ê³µí†µ ë°ì´í„° ê´€ë¦¬ í•¨ìˆ˜ (Data Loader)
# =========================================================

# [ìˆ˜ì •] ë°ì´í„° ë¡œë”© í•¨ìˆ˜ (ìºì‹± ì ìš©)
@st.cache_data
def load_data(file_path):
    """
    CSV íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    Streamlitì˜ ìºì‹œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ì†ë„ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.
    """
    if not os.path.exists(file_path):
        return None
    
    try:
        # utf-8ë¡œ ë¨¼ì € ì‹œë„í•˜ê³  ì‹¤íŒ¨í•˜ë©´ euc-krë¡œ ì‹œë„
        df = pd.read_csv(file_path, encoding='utf-8')
    except UnicodeDecodeError:
        df = pd.read_csv(file_path, encoding='euc-kr')
        
    return df

@st.cache_data
def get_company_data():
    """ê¸°ì—… ìˆœìœ„, ìœ„ì¹˜, ìƒì„¸ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    # 1. ì§€ë„ ë° ì°¨íŠ¸ìš© ë°ì´í„°
    data_map = {
        "ìˆœìœ„": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
        "ê¸°ì—…ëª…": ["ë†ì‹¬", "ì˜¤ë¦¬ì˜¨", "CJì œì¼ì œë‹¹", "ì‚¼ì–‘ì‹í’ˆ", "í’€ë¬´ì›", 
                "ë¹™ê·¸ë ˆ", "ë§¤ì¼ìœ ì—…", "í•˜ì´íŠ¸ì§„ë¡œ", "ë¡¯ë°ì¹ ì„±ìŒë£Œ", "ëŒ€ìƒ"],
        "ì´ì ": [177, 163, 159, 152, 152, 149, 142, 140, 132, 126],
        "ì£¼ì†Œ": [
            "ì„œìš¸ ë™ì‘êµ¬ ì—¬ì˜ëŒ€ë°©ë¡œ 112", "ì„œìš¸ ìš©ì‚°êµ¬ ë°±ë²”ë¡œ 90ë‹¤ê¸¸ 13", "ì„œìš¸ ì¤‘êµ¬ ë™í˜¸ë¡œ 330", 
            "ì„œìš¸ ì¢…ë¡œêµ¬ ì¢…ë¡œ33ê¸¸ 31", "ì¶©ë¶ ìŒì„±êµ° ëŒ€ì†Œë©´ ì‚¼ì–‘ë¡œ 730-27", "ì„œìš¸ ì¢…ë¡œêµ¬ ìƒˆë¬¸ì•ˆë¡œ 76",
            "ì„œìš¸ ì¢…ë¡œêµ¬ ì¢…ë¡œ1ê¸¸ 50", "ì„œìš¸ ê°•ì„œêµ¬ ê³µí•­ëŒ€ë¡œ 49", "ì„œìš¸ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 521", "ì„œìš¸ ì¢…ë¡œêµ¬ ì°½ê²½ê¶ë¡œ 120"
        ],
        "lat": [37.51008, 37.53584, 37.46575, 37.57694, 36.61402, 
                37.56975, 37.56789, 37.56934, 37.47320, 37.57644],
        "lon": [126.96212, 126.97442, 126.97150, 126.99550, 127.08162, 
                126.98507, 126.97555, 126.85240, 127.06268, 127.00220]
    }
    df_map = pd.DataFrame(data_map)

    # 2. ê¸°ì—… ìƒì„¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    company_details = [
        {
            "ìˆœìœ„": 1, "ê¸°ì—…ëª…": "ë†ì‹¬", 
            "ì†Œê°œ": "1968ë…„ ì„¤ë¦½, ë¼ë©´Â·ìŠ¤ë‚µÂ·ìŒë£Œì˜ êµ­ë‚´ 1ìœ„ ì œì¡°ê¸°ì—….",
            "ì£¼ë ¥ì œí’ˆ": "ì‹ ë¼ë©´, ì•ˆì„±íƒ•ë©´, ì§œíŒŒê²Œí‹°, ë„ˆêµ¬ë¦¬, ìƒˆìš°ê¹¡",
            "ë¹„ì „": "ê¸€ë¡œë²Œ ë¼ë©´ ì‹œì¥ í™•ëŒ€, ìŠ¤ë§ˆíŠ¸íŒœ ê¸°ìˆ  ë„ì…",
            "í™ˆí˜ì´ì§€": "https://www.nongshim.com", "ìœ íŠœë¸Œ": "https://www.youtube.com/@nongshim"
        },
        {
            "ìˆœìœ„": 2, "ê¸°ì—…ëª…": "ì˜¤ë¦¬ì˜¨", 
            "ì†Œê°œ": "1974ë…„ ì´ˆì½”íŒŒì´ ì¶œì‹œ. êµ­ë‚´ ì œê³¼ì—…ê³„ì˜ ëŒ€í‘œê¸°ì—….",
            "ì£¼ë ¥ì œí’ˆ": "ì´ˆì½”íŒŒì´, í¬ì¹´ì¹©, ì˜¤ì§•ì–´ë•…ì½©, ë‹¥í„°ìœ ",
            "ë¹„ì „": "ê¸€ë¡œë²Œ ì‹œì¥ ì‹¬í™”, ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ í™•ëŒ€",
            "í™ˆí˜ì´ì§€": "https://www.orionworld.com", "ìœ íŠœë¸Œ": "https://www.youtube.com/@ORIONworld"
        },
        {
            "ìˆœìœ„": 3, "ê¸°ì—…ëª…": "CJì œì¼ì œë‹¹", 
            "ì†Œê°œ": "êµ­ë‚´ ìµœëŒ€ ì‹í’ˆíšŒì‚¬. ì‹í’ˆÂ·ì œì•½Â·ë¬¼ë¥˜Â·ë°”ì´ì˜¤ ë“± ì‚¬ì—… ë‹¤ê°í™”.",
            "ì£¼ë ¥ì œí’ˆ": "ë°±ì„¤, ë‹¤ì‹œë‹¤, í–‡ë°˜, ë¹„ë¹„ê³ ",
            "ë¹„ì „": "ê¸€ë¡œë²Œ ì‹í’ˆê¸°ì—… ë„ì•½, K-í‘¸ë“œ ì„¸ê³„í™”",
            "í™ˆí˜ì´ì§€": "https://www.cj.net", "ìœ íŠœë¸Œ": "https://www.youtube.com/@CJCheilJedangOfficial"
        },
        {
            "ìˆœìœ„": 4, "ê¸°ì—…ëª…": "ì‚¼ì–‘ì‹í’ˆ", 
            "ì†Œê°œ": "ë¶ˆë‹­ë³¶ìŒë©´ì˜ ê¸€ë¡œë²Œ ì„±ê³µìœ¼ë¡œ ê¸‰ì„±ì¥í•œ ë¼ë©´ ë° ì‹í’ˆ ê¸°ì—….",
            "ì£¼ë ¥ì œí’ˆ": "ì‚¼ì–‘ë¼ë©´, ë¶ˆë‹­ë³¶ìŒë©´",
            "ë¹„ì „": "ê¸€ë¡œë²Œ ì¢…í•©ì‹í’ˆ ê¸°ì—… ë„ì•½",
            "í™ˆí˜ì´ì§€": "https://www.samyangfoods.com", "ìœ íŠœë¸Œ": "https://www.youtube.com/@samyangfoods"
        },
        {
            "ìˆœìœ„": 5, "ê¸°ì—…ëª…": "í’€ë¬´ì›", 
            "ì†Œê°œ": "ë°”ë¥¸ ë¨¹ê±°ë¦¬ ì›ì¹™ì„ ì§€í‚¤ëŠ” ë¡œí•˜ìŠ¤(LOHAS) ì„ ë„ ê¸°ì—….",
            "ì£¼ë ¥ì œí’ˆ": "ë‘ë¶€, ì½©ë‚˜ë¬¼, ì–„í”¼ë§Œë‘, ì§€êµ¬ì‹ë‹¨",
            "ë¹„ì „": "ì‹ë¬¼ì„± ì§€í–¥ ì‹í’ˆ í™•ëŒ€, ì§€ì†ê°€ëŠ¥ê²½ì˜",
            "í™ˆí˜ì´ì§€": "https://www.pulmuone.co.kr", "ìœ íŠœë¸Œ": "https://www.youtube.com/@pulmuone.official"
        },
        {
            "ìˆœìœ„": 6, "ê¸°ì—…ëª…": "ë¹™ê·¸ë ˆ", 
            "ì†Œê°œ": "ë°”ë‚˜ë‚˜ë§›ìš°ìœ , ìš”í”Œë ˆ ë“± ìœ ê°€ê³µ ë° ì•„ì´ìŠ¤í¬ë¦¼ ì „ë¬¸ ê¸°ì—….",
            "ì£¼ë ¥ì œí’ˆ": "ë°”ë‚˜ë‚˜ë§›ìš°ìœ , ìš”í”Œë ˆ, íˆ¬ê²Œë”, ë©”ë¡œë‚˜",
            "ë¹„ì „": "ê¸€ë¡œë²Œ ë¹„ì¦ˆë‹ˆìŠ¤ í™•ëŒ€, í”„ë¦¬ë¯¸ì—„ ì œí’ˆ ê°•í™”",
            "í™ˆí˜ì´ì§€": "https://www.bing.co.kr", "ìœ íŠœë¸Œ": "https://www.youtube.com/@official.binggrae"
        },
        {
            "ìˆœìœ„": 7, "ê¸°ì—…ëª…": "ë§¤ì¼ìœ ì—…", 
            "ì†Œê°œ": "ìš°ìœ , ë¶„ìœ , ì¹˜ì¦ˆ ë“± ìœ ì œí’ˆ ì „ë¬¸ ê¸°ì—…. ì„±ì¸ ì˜ì–‘ì‹ ì…€ë ‰ìŠ¤ ë³´ìœ .",
            "ì£¼ë ¥ì œí’ˆ": "ë§¤ì¼ìš°ìœ , ìƒí•˜ëª©ì¥, ì•±ì†”ë£¨íŠ¸, ì…€ë ‰ìŠ¤",
            "ë¹„ì „": "ìƒì• ì£¼ê¸°ë³„ ë§ì¶¤í˜• ì˜ì–‘ ì„¤ê³„, ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ê°•í™”",
            "í™ˆí˜ì´ì§€": "https://www.maeil.com", "ìœ íŠœë¸Œ": "https://www.youtube.com/@maeili2mo"
        },
        {
            "ìˆœìœ„": 8, "ê¸°ì—…ëª…": "í•˜ì´íŠ¸ì§„ë¡œ", 
            "ì†Œê°œ": "ëŒ€í•œë¯¼êµ­ ëŒ€í‘œ ì£¼ë¥˜ ê¸°ì—…. ì†Œì£¼ì™€ ë§¥ì£¼ ì‹œì¥ì˜ ê°•ì.",
            "ì£¼ë ¥ì œí’ˆ": "ì°¸ì´ìŠ¬, ì§„ë¡œ, í…Œë¼, ì¼ˆë¦¬",
            "ë¹„ì „": "ê¸€ë¡œë²Œ ì£¼ë¥˜ ê¸°ì—… ë„ì•½, ESG ê²½ì˜ ê°•í™”",
            "í™ˆí˜ì´ì§€": "https://www.hitejinro.com", "ìœ íŠœë¸Œ": "https://www.youtube.com/watch?v=CjYD_J_2tt0"
        },
        {
            "ìˆœìœ„": 9, "ê¸°ì—…ëª…": "ë¡¯ë°ì¹ ì„±ìŒë£Œ", 
            "ì†Œê°œ": "ìŒë£Œ ë° ì£¼ë¥˜ ì „ë¬¸ ê¸°ì—…. ì¹ ì„±ì‚¬ì´ë‹¤ì™€ ì²˜ìŒì²˜ëŸ¼ ë³´ìœ .",
            "ì£¼ë ¥ì œí’ˆ": "ì¹ ì„±ì‚¬ì´ë‹¤, í©ì‹œ, ì²˜ìŒì²˜ëŸ¼, ìƒˆë¡œ",
            "ë¹„ì „": "Zì„¸ëŒ€ íƒ€ê²Ÿ ë§ˆì¼€íŒ… ê°•í™”, í—¬ìŠ¤ì¼€ì–´ í¬íŠ¸í´ë¦¬ì˜¤ í™•ëŒ€",
            "í™ˆí˜ì´ì§€": "https://company.lottechilsung.co.kr", "ìœ íŠœë¸Œ": "https://www.youtube.com/@Lotte7star"
        },
        {
            "ìˆœìœ„": 10, "ê¸°ì—…ëª…": "ëŒ€ìƒ", 
            "ì†Œê°œ": "ì²­ì •ì›, ì¢…ê°€ì§‘ ë¸Œëœë“œë¥¼ ë³´ìœ í•œ ì¢…í•© ì‹í’ˆ ê¸°ì—….",
            "ì£¼ë ¥ì œí’ˆ": "ì²­ì •ì›, ë¯¸ì›, ì¢…ê°€ì§‘ ê¹€ì¹˜",
            "ë¹„ì „": "ê¸€ë¡œë²Œ í•œì‹ ëŒ€í‘œ ë¸Œëœë“œ ìœ¡ì„±",
            "í™ˆí˜ì´ì§€": "https://www.daesang.com", "ìœ íŠœë¸Œ": "https://www.youtube.com/@DAESANG"
        }
    ]

    return df_map, company_details

# =========================================================
# 1. í¬íŠ¸í´ë¦¬ì˜¤ ì†Œê°œ (Intro)
# =========================================================

def page_intro():
    st.title("ğŸ™‹â€â™‚ï¸ ìœµí•© ì¸ì¬ í¬íŠ¸í´ë¦¬ì˜¤")
    st.caption("ì‹í’ˆìƒëª…ê³µí•™ x ê²½ì œ x ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤")

    tab1, tab2, tab3 = st.tabs(["ğŸ™‹â€â™‚ï¸ í”„ë¡œí•„ & ê´€ì‹¬ì‚¬", "ğŸ“š ìˆ˜ê°• ë° í•™ìŠµ í˜„í™©", "ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ"])

    with tab1:
        st.header("Who am I?")
        col1, col2 = st.columns([1, 2])
        with col1:
            st.markdown("<h1 style='text-align: center;'>ğŸ‘¨â€ğŸ”¬</h1>", unsafe_allow_html=True)
        with col2:
            st.markdown("""
            - **ì´ë¦„:** ì •ì§€í˜¸ (02ë…„ìƒ)
            - **ì „ê³µ:** ì‹í’ˆìƒëª…ê³µí•™ (21í•™ë²ˆ, 4í•™ë…„)
            - **ê´€ì‹¬ ë¶„ì•¼:** ì‹í’ˆ R&D, ë°ì´í„° ë¶„ì„, ê²½ì œ ë™í–¥
            """)

        st.divider()
        st.subheader("ğŸ“¢ ìê¸°ì†Œê°œ")
        st.write("""
        ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” **ì‹í’ˆìƒëª…ê³µí•™**ì„ ì „ê³µí•˜ê³  ìˆëŠ” 4í•™ë…„ ì •ì§€í˜¸ì…ë‹ˆë‹¤.
        ë³¸ í•™ê¸°ì—ëŠ” ì „ê³µ ì§€ì‹ì„ ë„“íˆê¸° ìœ„í•´ **ê²½ì œ** ê´€ë ¨ ì—°ê³„ ì „ê³µ ìˆ˜ì—…ê³¼ **'ì»´í“¨íŒ… íƒìƒ‰'** ë“± IT ìˆ˜ì—…ì„ í•¨ê»˜ ìˆ˜ê°•í•˜ê³  ìˆìŠµë‹ˆë‹¤.
        
        í˜„ì¬ ê°€ì¥ í° ê³ ë¯¼ì€ **ì¡¸ì—… í›„ ì§„ë¡œ**ì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ ì‹í’ˆì„ ì—°êµ¬í•˜ëŠ” ê²ƒì„ ë„˜ì–´, 
        ë°ì´í„°ì™€ ê²½ì œì  ê´€ì ì„ ê²°í•©í•˜ì—¬ ì‹œì¥ì´ ì›í•˜ëŠ” ì¸ì¬ê°€ ë˜ê¸° ìœ„í•´ ì¹˜ì—´í•˜ê²Œ ê³ ë¯¼í•˜ê³  ìˆìŠµë‹ˆë‹¤.
        """)

        st.subheader("ğŸ’– ì¢‹ì•„í•˜ëŠ” ê²ƒ")
        with st.expander("ğŸ° ì €ì˜ ë‹¬ì½¤í•œ ì·¨ë¯¸ ë³´ëŸ¬ê°€ê¸° (Click!)"):
            st.write("""
            ì œê°€ ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ê²ƒì€ **ë””ì €íŠ¸**ì…ë‹ˆë‹¤. ì˜ ë§Œë“¤ì–´ì§„ ì¼€ì´í¬ì™€ ìŒë£Œ í•œ ì”ì€ í° í–‰ë³µì„ ì¤ë‹ˆë‹¤.
            ë³¸ê°€ì—ì„œëŠ” ë§ˆë“¤ë Œ, íœ˜ë‚­ì‹œì— ê°™ì€ êµ¬ì›€ê³¼ìë¥¼ ì§ì ‘ ë§Œë“¤ì–´ ë¨¹ìœ¼ë©° ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ í•´ì†Œí•˜ê³¤ í•©ë‹ˆë‹¤.
            """)
            st.info("ğŸ‘‡ ëŒ€ë¦¬ë§Œì¡±ì„ ìœ„í•´ ìì£¼ ë³´ëŠ” ì±„ë„")
            st.link_button("ìœ íŠœë²„ 'ë¹µë”˜' ë³´ëŸ¬ê°€ê¸°", "https://www.youtube.com/@ë¹µë”˜")

    with tab2:
        st.header("Academic Roadmap")
        col1, col2, col3 = st.columns(3)
        col1.metric("í˜„ì¬ í•™ë…„", "4í•™ë…„")
        col2.metric("ì´ë²ˆ í•™ê¸° ìˆ˜ê°•", "4ê³¼ëª©", "ìœµí•© í•™ìŠµ")
        col3.metric("ì´ í•™ì ", "12í•™ì ", "-6 (ì§‘ì¤‘ í•™ê¸°)", delta_color="inverse")

        st.divider()
        st.subheader("ğŸ“… ì´ë²ˆ í•™ê¸° ì‹œê°„í‘œ")
        data = {
            "êµì‹œ": ["1êµì‹œ", "2êµì‹œ"],
            "ì›”": ["ì»´í“¨íŒ… í•µì‹¬", ""],
            "í™”": ["ê±°ì‹œê²½ì œì´ë¡ ", ""],
            "ìˆ˜": ["ì»´í“¨íŒ… í•µì‹¬", "ë¯¸ì‹œê²½ì œì´ë¡ "],
            "ëª©": ["", ""],
            "ê¸ˆ": ["ì»´í“¨íŒ… íƒìƒ‰", ""]
        }
        st.table(pd.DataFrame(data))

        st.subheader("ğŸ” ìˆ˜ì—… ìƒì„¸ ì •ë³´")
        st.caption("JSON íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ í†µí•´ ë°ì´í„° êµ¬ì¡°í™” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        json_data = {
            "ì»´í“¨íŒ… íƒìƒ‰": {"êµìˆ˜": "ë³€í•´ì„ ", "ê°•ì˜ì‹¤": "26ë™ 104í˜¸", "ìœ í˜•": "êµì–‘"},
            "ì»´í“¨íŒ… í•µì‹¬": {"êµìˆ˜": "ê¹€í˜„ì£¼", "ê°•ì˜ì‹¤": "26ë™ 104í˜¸", "ìœ í˜•": "êµì–‘"},
            "ë¯¸ì‹œê²½ì œì´ë¡ ": {"êµìˆ˜": "Gueron Yves", "ê°•ì˜ì‹¤": "16ë™ 110í˜¸", "ìœ í˜•": "ì—°ê³„ì „ê³µ"},
            "ê±°ì‹œê²½ì œì´ë¡ ": {"êµìˆ˜": "ìµœì¬ì›", "ê°•ì˜ì‹¤": "223ë™ 107í˜¸", "ìœ í˜•": "ì—°ê³„ì „ê³µ"}
        }
        st.json(json_data)

    with tab3:
        st.header("Why this Project?")
        st.success("ì´ í”„ë¡œì íŠ¸ëŠ” ë§‰ì—°í•œ ì·¨ì—… ì‹œì¥ì„ ë°ì´í„°ë¥¼ í†µí•´ ëª…í™•í•˜ê²Œ ë¶„ì„í•˜ê¸° ìœ„í•´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        st.write("""
        **ì•ìœ¼ë¡œì˜ ëª©í‘œ:**
        ëŒ€í•™ ìƒí™œ ë™ì•ˆ ë‹¤ì–‘í•œ ê²½í—˜ì„ ìŒ“ê³  ìƒˆë¡œìš´ ì§€ì‹ì„ ìŠµë“í•˜ì—¬, ì¡¸ì—… í›„ ì œê°€ ì§„ì •ìœ¼ë¡œ ì›í•˜ëŠ” ê¸¸ì„ ì°¾ê³  ì‹¶ìŠµë‹ˆë‹¤.
        ì´ë²ˆ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ **ì‹í’ˆ ì‚°ì—…ì˜ íŠ¸ë Œë“œ**ì™€ **ì—°êµ¬ ë™í–¥**ì„ ì§ì ‘ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•˜ì—¬ ê·¸ í•´ë‹µì„ ì°¾ì•„ë³´ë ¤ í•©ë‹ˆë‹¤.
        """)
        
        st.subheader("ğŸ”§ ì‚¬ìš©ëœ ê¸°ìˆ  ìŠ¤íƒ")
        st.code("""
import streamlit as st        # ì›¹ ëŒ€ì‹œë³´ë“œ êµ¬í˜„
import pandas as pd           # ë°ì´í„° ì •ì œ ë° ë¶„ì„
from bs4 import BeautifulSoup # ì›¹ ë°ì´í„° ìˆ˜ì§‘ (í¬ë¡¤ë§)
import pydeck as pdk          # ì§€ë„ ì‹œê°í™”
        """, language="python")

        st.subheader("ğŸ“ˆ ë¶„ì„ ë°©ë²•ë¡  (ì˜ˆì‹œ)")
        st.write("ë°ì´í„° ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ í†µê³„ì  ì ‘ê·¼ì„ ì‹œë„í•  ì˜ˆì •ì…ë‹ˆë‹¤.")
        st.latex(r"Correlation(X, Y) = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum(x_i - \bar{x})^2 \sum(y_i - \bar{y})^2}}")

# =========================================================
# 2. êµ­ë‚´ ì‹í’ˆ íŠ¸ë Œë“œ ë¶„ì„ (Trend)
# =========================================================
def page_keyword_analysis():
    st.title("ğŸ“ˆ í‘¸ë“œ íŠ¸ë Œë“œ & í‚¤ì›Œë“œ ë¶„ì„")
    st.markdown("êµ¬ê¸€ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ **ì‹¤ì œ ì†Œë¹„ì ê´€ì‹¬ë„** ë³€í™”ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

    # [ìˆ˜ì •] íŒŒì¼ ê²½ë¡œ ì„¤ì • (GitHub ë°°í¬ ì‹œ ê²½ë¡œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ìƒëŒ€ ê²½ë¡œ ì‚¬ìš© ê¶Œì¥)
    csv_file = './food_trends.csv'
    
    # ìºì‹±ëœ í•¨ìˆ˜ë¥¼ í†µí•´ ë°ì´í„° ë¡œë“œ
    df = load_data(csv_file)

    if df is None:
        st.error(f"âš ï¸ '{csv_file}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.warning(f"í˜„ì¬ í´ë” ìœ„ì¹˜: {os.getcwd()}")
        st.info("Tip: GitHubì— ì˜¬ë¦´ ë•Œ 'food_trends.csv' íŒŒì¼ì´ app.pyì™€ ê°™ì€ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return

    try:
        # 1. ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜
        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'])
            df.set_index('Date', inplace=True)
        else:
            df.rename(columns={df.columns[0]: 'Date'}, inplace=True)
            df['Date'] = pd.to_datetime(df['Date'])
            df.set_index('Date', inplace=True)

        # 2. ë°ì´í„° ì „ì²˜ë¦¬
        for col in df.columns:
            if df[col].dtype == 'object':
                df[col] = df[col].astype(str).str.replace('<1', '0')
                df[col] = df[col].astype(str).str.replace(',', '')
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
            else:
                df[col] = df[col].fillna(0)
        
    except Exception as e:
        st.error(f"ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ” í‚¤ì›Œë“œ ì„ íƒ")
    
    keywords = df.columns.tolist()
    default_selection = keywords[:3] if len(keywords) > 0 else []
    
    selected_keywords = st.sidebar.multiselect(
        "ë¶„ì„í•  í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš”",
        keywords,
        default=default_selection
    )

    if not selected_keywords:
        st.warning("ë¶„ì„í•  í‚¤ì›Œë“œë¥¼ 1ê°œ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
        return

    # [ì‹œê°í™” 1] ì‹œê³„ì—´ ê·¸ë˜í”„
    st.subheader("ğŸ—“ï¸ ì£¼ê°„ ê´€ì‹¬ë„ ë³€í™” (Time Series)")
    if not df.empty:
        st.caption(f"ë¶„ì„ ê¸°ê°„: {df.index.min().date()} ~ {df.index.max().date()}")
    
    fig = px.line(
        df,
        y=selected_keywords,
        labels={"value": "ê²€ìƒ‰ ì§€ìˆ˜", "Date": "ë‚ ì§œ", "variable": "í‚¤ì›Œë“œ"},
        template="plotly_white"
    )
    fig.update_layout(hovermode="x unified")
    st.plotly_chart(fig, use_container_width=True)

    # [ì‹œê°í™” 2] ë°ì´í„° ìš”ì•½ (Metric)
    st.divider()
    st.subheader("ğŸ“Š ìµœê·¼ íŠ¸ë Œë“œ ìš”ì•½ (Last 4 Weeks)")
    
    cols = st.columns(len(selected_keywords))
    for i, key in enumerate(selected_keywords):
        if len(df) > 8:
            recent = df[key].iloc[-4:].mean()
            past = df[key].iloc[-8:-4].mean()
            diff = recent - past
        else:
            recent = df[key].mean()
            diff = 0
        
        with cols[i % 4]:
            st.metric(
                label=key,
                value=f"{recent:.1f}",
                delta=f"{diff:.1f}",
                help="ìµœê·¼ 4ì£¼ í‰ê·  ê²€ìƒ‰ëŸ‰ì…ë‹ˆë‹¤."
            )

    # [ì‹œê°í™” 3] ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
    st.divider()
    st.subheader("ğŸ”— í‚¤ì›Œë“œ ê°„ ìƒê´€ê´€ê³„ (Correlation)")
    if len(selected_keywords) >= 2:
        corr = df[selected_keywords].corr()
        fig_corr = px.imshow(corr, text_auto=".2f", color_continuous_scale="RdBu_r")
        st.plotly_chart(fig_corr, use_container_width=True)
    else:
        st.info("ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•´ 2ê°œ ì´ìƒì˜ í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš”.")

# =========================================================
# 3. ì‹í’ˆ ê¸°ì—… ê±°ì  ì§€ë„ (Map)
# =========================================================

def page_map_visualization():
    """ 3. ê¸°ì—… ìˆœìœ„ ë° ìœ„ì¹˜ ì‹œê°í™” í˜ì´ì§€ """
    df_map, _ = get_company_data()

    st.title("ğŸ—ºï¸ ì‹í’ˆ ê¸°ì—… 10ëŒ€ ê±°ì  ì§€ë„")
    st.info("K-Brand Index ìƒìœ„ 10ê°œ ê¸°ì—…ì˜ ìœ„ì¹˜ì™€ ë¸Œëœë“œ í‰íŒ ìˆœìœ„ë¥¼ ì‹œê°í™”í–ˆìŠµë‹ˆë‹¤.")

    # --- 1. ê¸°ì—… ìˆœìœ„ ì‹œê°í™” ---
    st.subheader("ğŸ“Š K-Brand Index ì‹í’ˆ ë¶€ë¬¸ TOP 10")
    st.markdown("""
    - **ì¶œì²˜:** ì•„ì‹œì•„ë¸Œëœë“œì—°êµ¬ì†Œ (2025.11.01 ~ 11.30)
    - **ì§€í‘œ:** ë¹…ë°ì´í„° ì‹œìŠ¤í…œ ì˜¨ë¼ì¸ ì¸ë±ìŠ¤ ìˆ˜ì¹˜ í•©ì‚°
    """)

    fig = px.bar(
        df_map, 
        x="ì´ì ", 
        y="ê¸°ì—…ëª…", 
        orientation='h', 
        text="ì´ì ", 
        color="ì´ì ", 
        color_continuous_scale="Bluered", 
        title="ê¸°ì—…ë³„ ë¸Œëœë“œ í‰íŒ ì´ì  ë¹„êµ"
    )
    fig.update_layout(yaxis={'categoryorder':'total ascending'}) 
    st.plotly_chart(fig, use_container_width=True)

    st.divider()

    # --- 2. ì§€ë„ ì‹œê°í™” (PyDeck) ---
    st.subheader("ğŸ“ ë³¸ì‚¬ ìœ„ì¹˜ ì‹œê°í™”")
    st.caption("ì§€ë„ì˜ ì ì„ í´ë¦­í•˜ê±°ë‚˜ ë§ˆìš°ìŠ¤ë¥¼ ì˜¬ë¦¬ë©´ ê¸°ì—…ëª…ê³¼ ì£¼ì†Œê°€ í‘œì‹œë©ë‹ˆë‹¤.")

    layer = pdk.Layer(
        "ScatterplotLayer",
        data=df_map,
        get_position='[lon, lat]',
        get_radius=2000,
        get_fill_color='[255, 0, 0, 180]',
        pickable=True,
        stroked=True,
        filled=True
    )

    view_state = pdk.ViewState(
        latitude=36.5,
        longitude=127.5, 
        zoom=6,
        pitch=0
    )

    tooltip = {
        "html": "<b>{ìˆœìœ„}ìœ„ {ê¸°ì—…ëª…}</b><br>ì´ì : {ì´ì }ì <br>ì£¼ì†Œ: {ì£¼ì†Œ}",
        "style": {"backgroundColor": "steelblue", "color": "white"}
    }

    st.pydeck_chart(pdk.Deck(
        layers=[layer],
        initial_view_state=view_state,
        tooltip=tooltip
    ))

# =========================================================
# 4. ì‹í’ˆ ê¸°ì—… ìƒì„¸ ì •ë³´ (Info)
# =========================================================

def page_company_info():
    """ 4. ê¸°ì—…ë³„ ìƒì„¸ ì •ë³´ í˜ì´ì§€ """
    _, company_details = get_company_data()

    st.title("ğŸ¢ 10ëŒ€ ì‹í’ˆ ê¸°ì—… ìƒì„¸ ì •ë³´")
    st.info("ê° ê¸°ì—…ì˜ ì£¼ìš” ë¹„ì „, ì£¼ë ¥ ì œí’ˆ ë° ê³µì‹ ì±„ë„ ë§í¬ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    
    st.markdown("---")

    for i in range(0, len(company_details), 2):
        cols = st.columns(2)
        with cols[0]:
            c1 = company_details[i]
            with st.expander(f"**{c1['ìˆœìœ„']}ìœ„. {c1['ê¸°ì—…ëª…']}**", expanded=True):
                st.markdown(f"**ğŸ’¡ ì†Œê°œ:** {c1['ì†Œê°œ']}")
                st.markdown(f"**ğŸ›’ ì œí’ˆ:** {c1['ì£¼ë ¥ì œí’ˆ']}")
                st.markdown(f"**ğŸš€ ë¹„ì „:** {c1['ë¹„ì „']}")
                st.markdown("---")
                if c1.get("í™ˆí˜ì´ì§€"):
                    st.link_button("ğŸ  í™ˆí˜ì´ì§€", c1["í™ˆí˜ì´ì§€"], use_container_width=True)
                if c1.get("ìœ íŠœë¸Œ"):
                    st.link_button("ğŸ“º ìœ íŠœë¸Œ ì±„ë„", c1["ìœ íŠœë¸Œ"], use_container_width=True)
        
        if i + 1 < len(company_details):
            with cols[1]:
                c2 = company_details[i+1]
                with st.expander(f"**{c2['ìˆœìœ„']}ìœ„. {c2['ê¸°ì—…ëª…']}**", expanded=True):
                    st.markdown(f"**ğŸ’¡ ì†Œê°œ:** {c2['ì†Œê°œ']}")
                    st.markdown(f"**ğŸ›’ ì œí’ˆ:** {c2['ì£¼ë ¥ì œí’ˆ']}")
                    st.markdown(f"**ğŸš€ ë¹„ì „:** {c2['ë¹„ì „']}")
                    st.markdown("---")
                    if c2.get("í™ˆí˜ì´ì§€"):
                        st.link_button("ğŸ  í™ˆí˜ì´ì§€", c2["í™ˆí˜ì´ì§€"], use_container_width=True)
                    if c2.get("ìœ íŠœë¸Œ"):
                        st.link_button("ğŸ“º ìœ íŠœë¸Œ ì±„ë„", c2["ìœ íŠœë¸Œ"], use_container_width=True)

# =========================================================
# 5. ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„ (Research)
# =========================================================

def page_scholar_analysis():
    """ 5. ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„ í˜ì´ì§€ (Google Scholar Crawling - Year Extraction Fix) """
    st.title("ğŸ“ ì—°êµ¬ íŠ¸ë Œë“œ ì‹¬ì¸µ ë¶„ì„")
    st.markdown("""
    êµ¬ê¸€ ìŠ¤ì¹¼ë¼(Google Scholar)ì—ì„œ **ë‹¤ì¤‘ í˜ì´ì§€ í¬ë¡¤ë§**ì„ í†µí•´ ë” í’ë¶€í•œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    (ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì—°ë„ë³„ íŠ¸ë Œë“œì™€ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.)
    """)

    # 1. í‚¤ì›Œë“œ ì…ë ¥
    st.subheader("ğŸ” Search Keywords")
    
    col1, col2 = st.columns([1, 2])
    with col1:
        st.markdown("**Recommended Keywords**")
        keywords = [
            "Food Chemistry", "Food Microbiology", "Food Engineering", 
            "Functional Food", "Fermentation Technology", "Food Safety", 
            "Food Nutrition", "Biotechnology", "Microbiome", "Alternative Meat"
        ]
        selected_keyword = st.selectbox("Select a keyword", keywords)
    
    with col2:
        query = st.text_input("Or type your own keyword", value=selected_keyword)

    pages_to_crawl = st.slider("í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ (í˜ì´ì§€ë‹¹ 10ê°œ)", 1, 5, 3)

    run_search = st.button("ğŸš€ Start Analysis (Data Collection)")

    if run_search and query:
        st.divider()
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # ë°ì´í„° ì €ì¥ì†Œ
        all_titles = []
        all_years = []

        try:
            chromedriver_autoinstaller.install()
            
            options = Options()
            # ë´‡ íƒì§€ íšŒí”¼ ì˜µì…˜
            options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36")
            options.add_argument("--headless") 
            options.add_argument("--disable-gpu")
            
            driver = webdriver.Chrome(options=options)
            
            # í˜ì´ì§€ ë°˜ë³µ í¬ë¡¤ë§
            for i in range(pages_to_crawl):
                start_index = i * 10
                status_text.info(f"â³ '{query}' ê´€ë ¨ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤... ({i+1}/{pages_to_crawl} í˜ì´ì§€)")
                
                url = f"https://scholar.google.co.kr/scholar?start={start_index}&q={query}&hl=en&as_sdt=0,5"
                driver.get(url)
                
                time.sleep(2 + random.random()) # random delay
                driver.implicitly_wait(5)

                html = driver.page_source
                soup = BeautifulSoup(html, "html.parser")
                
                results = soup.find_all("div", class_="gs_r gs_or gs_scl")
                
                for row in results:
                    title_tag = row.find("h3", class_="gs_rt")
                    
                    # ì œëª©ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì—°ë„ ì°¾ê¸°ë¥¼ ì‹œë„ (ë°ì´í„° ì§ ë§ì¶”ê¸° ìœ„í•¨)
                    if title_tag:
                        # 1. ì œëª© ì¶”ì¶œ
                        clean_title = title_tag.text.replace("[PDF]", "").replace("[HTML]", "").replace("[BOOK]", "").replace("[B]", "").strip()
                        all_titles.append(clean_title)
                        
                        # 2. ì—°ë„ ì¶”ì¶œ (ì œëª©ì— ëŒ€ì‘í•˜ëŠ” ì—°ë„ë¥¼ ì°¾ê±°ë‚˜, ì—†ìœ¼ë©´ None ì €ì¥)
                        meta_tag = row.find("div", class_="gs_a")
                        year_val = None # ê¸°ë³¸ê°’
                        
                        if meta_tag:
                            # 19xx ë˜ëŠ” 20xx í˜•íƒœì˜ 4ìë¦¬ ìˆ«ìë¥¼ ëª¨ë‘ ì°¾ìŒ
                            years_found = re.findall(r'(19\d{2}|20\d{2})', meta_tag.get_text())
                            if years_found:
                                # ì—¬ëŸ¬ ìˆ«ìê°€ ë‚˜ì˜¬ ê²½ìš° ë³´í†µ ë§¨ ë’¤ì— ë‚˜ì˜¤ëŠ” ê²ƒì´ ì¶œíŒ ì—°ë„ì¼ í™•ë¥ ì´ ë†’ìŒ
                                try:
                                    year_val = int(years_found[-1])
                                except:
                                    year_val = None
                        
                        # ì—°ë„ë¥¼ ì°¾ì•˜ë“  ëª» ì°¾ì•˜ë“  ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (ì œëª©ê³¼ ê¸¸ì´ ë§ì¶”ê¸°)
                        all_years.append(year_val)
                
                progress_bar.progress((i + 1) / pages_to_crawl)
            
            driver.quit()

            if all_titles:
                # Noneì´ ì•„ë‹Œ ì‹¤ì œ ì—°ë„ ë°ì´í„°ë§Œ í•„í„°ë§í•˜ì—¬ ì¹´ìš´íŠ¸
                valid_years = [y for y in all_years if y is not None]
                
                status_text.success(f"âœ… ë¶„ì„ ì™„ë£Œ! ì´ {len(all_titles)}ê±´ ì¤‘ {len(valid_years)}ê±´ì˜ ì—°ë„ ì •ë³´ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
                
                # 1. ì—°ë„ë³„ íŠ¸ë Œë“œ ì°¨íŠ¸
                st.subheader(f"ğŸ“Š Research Trends by Year ({query})")
                
                if valid_years:
                    year_counts = Counter(valid_years)
                    df_trend = pd.DataFrame(list(year_counts.items()), columns=['Year', 'Count'])
                    df_trend = df_trend.sort_values('Year')
                    
                    # ìµœê·¼ ë°ì´í„° ìœ„ì£¼ë¡œ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì •ë ¬
                    fig = px.bar(
                        df_trend, 
                        x='Year', 
                        y='Count',
                        text='Count',
                        title=f"Annual Publication Count for '{query}'",
                        labels={'Count': 'Number of Papers', 'Year': 'Year'},
                        template='plotly_white',
                        color='Count',
                        color_continuous_scale='Blues'
                    )
                    fig.update_traces(textposition='outside')
                    fig.update_layout(xaxis=dict(type='category')) # Xì¶•ì„ ì¹´í…Œê³ ë¦¬ë¡œ ì„¤ì •í•˜ì—¬ ì •ìˆ˜ë§Œ í‘œì‹œ
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.warning("âš ï¸ ì—°ë„ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (êµ¬ê¸€ ìŠ¤ì¹¼ë¼ í˜ì´ì§€ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆê±°ë‚˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.)")

                # 2. ì›Œë“œ í´ë¼ìš°ë“œ
                st.subheader(f"â˜ï¸ Key Topics Word Cloud")
                
                all_text = " ".join(all_titles)
                stopwords = {"of", "and", "the", "in", "a", "for", "on", "with", "to", "at", "by", "an", "analysis", "study", "review", "using", "based", "effect", "effects", "application", "applications"}
                
                wc = WordCloud(
                    font_path=font_path,
                    width=800,
                    height=400,
                    background_color="white",
                    colormap="viridis",
                    stopwords=stopwords
                ).generate(all_text)
                
                fig_wc, ax = plt.subplots(figsize=(10, 5))
                ax.imshow(wc, interpolation='bilinear')
                ax.axis("off")
                st.pyplot(fig_wc)
                
                # 3. ë…¼ë¬¸ ëª©ë¡ (ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹œ ê¸¸ì´ ë¶ˆì¼ì¹˜ ì˜¤ë¥˜ ë°©ì§€)
                with st.expander("ğŸ“œ View Collected Papers List"):
                    df_papers = pd.DataFrame({
                        "Title": all_titles,
                        "Year": all_years 
                    })
                    # ì—°ë„ê°€ ì—†ëŠ”(None) í–‰ì€ ë§¨ ì•„ë˜ë¡œ ë³´ë‚´ê±°ë‚˜ í‘œì‹œ
                    st.dataframe(df_papers.sort_values(by="Year", ascending=False, na_position='last'))
            
            else:
                status_text.error("ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (Google Scholar ë´‡ íƒì§€ ê°€ëŠ¥ì„±)")
                st.info("ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜, í¬ë¡¤ë§ í˜ì´ì§€ ìˆ˜ë¥¼ ì¤„ì—¬ë³´ì„¸ìš”.")

        except Exception as e:
            st.error(f"Error occurred: {e}")
            

# =========================================================
# 6. ê²°ë¡  ë° ì œì–¸ (Conclusion)
# =========================================================

def page_conclusion():
    st.title("ğŸ“ ê²°ë¡  ë° ì œì–¸ (Conclusion & Suggestion)")
    st.markdown("""
    ë³¸ í¬íŠ¸í´ë¦¬ì˜¤ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ì‹í’ˆ ì‚°ì—…ì˜ í˜„ì¬ íŠ¸ë Œë“œë¥¼ ë°ì´í„°ë¥¼ í†µí•´ ì •ëŸ‰ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ , 
    ë¯¸ë˜ ì‹í’ˆ ì‚°ì—…ì—ì„œì˜ ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì • ê°€ëŠ¥ì„±ì„ êµ¬ì²´ì ìœ¼ë¡œ íƒìƒ‰í–ˆìŠµë‹ˆë‹¤.
    """)

    st.subheader("1. ë¶„ì„ ìš”ì•½ (Summary of Analysis)")
    col1, col2 = st.columns(2)
    with col1:
        st.info("**ğŸ“ˆ íŠ¸ë Œë“œ(Trend) ë¶„ì„ ê²°ë¡ **")
        st.markdown("""
        - **ê±´ê°• ì§€í–¥ì„± ì‹¬í™”**: 'ì €ì†ë…¸í™”', 'ì œë¡œìŠˆê±°', 'ë‹¨ë°±ì§ˆ' í‚¤ì›Œë“œì˜ ê²€ìƒ‰ëŸ‰ì´ ê¾¸ì¤€íˆ ìƒìœ„ê¶Œì„ ìœ ì§€í•˜ë©°, ì†Œë¹„ìë“¤ì´ ë‹¨ìˆœí•œ ë§›ì„ ë„˜ì–´ **'ê¸°ëŠ¥ì„±'ê³¼ 'ê±´ê°•'**ì„ ì‹í’ˆ ì„ íƒì˜ ìµœìš°ì„  ê°€ì¹˜ë¡œ ë‘ê³  ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.
        - **SNS ë°”ì´ëŸ´ì˜ ì˜í–¥ë ¥**: 'ë‘ë°”ì´ì´ˆì½œë¦¿', 'ìš”ì•„ì •'ê³¼ ê°™ì€ í‚¤ì›Œë“œì˜ ê¸‰ë“±ë½ íŒ¨í„´ì€ í˜„ëŒ€ ì‹í’ˆ ì‹œì¥ì—ì„œ **SNS ìˆí¼ ì½˜í…ì¸ ì™€ ì‹œê°ì  ìš”ì†Œ**ê°€ íŠ¸ë Œë“œ í˜•ì„±ì— ê²°ì •ì ì¸ ì—­í• ì„ í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
        """)
    with col2:
        st.success("**ğŸ“ ì—°êµ¬(Research) ë™í–¥ ê²°ë¡ **")
        st.markdown("""
        - **ìœµí•© ê¸°ìˆ ì˜ ë¶€ìƒ**: í•™ìˆ  ê²€ìƒ‰ ê²°ê³¼, **Microbiome(ë§ˆì´í¬ë¡œë°”ì´ì˜´)**ê³¼ **Alternative Meat(ëŒ€ì²´ìœ¡)** ë¶„ì•¼ì˜ ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ê°€ ìµœê·¼ 3ë…„ê°„ ê¾¸ì¤€íˆ ì¦ê°€ ì¶”ì„¸ì…ë‹ˆë‹¤.
        - **ë¯¸ë˜ ë°©í–¥ì„±**: ì´ëŠ” ì‹í’ˆ ê³µí•™ì´ ë‹¨ìˆœ ê°€ê³µ ê¸°ìˆ ì„ ë„˜ì–´ **ë°”ì´ì˜¤/ìƒëª…ê³µí•™ ê¸°ìˆ **ê³¼ ìœµí•©ë˜ê³  ìˆìœ¼ë©°, ê°œì¸ ë§ì¶¤í˜• ì˜ì–‘(Personalized Nutrition) ì‹œëŒ€ë¡œ ë‚˜ì•„ê°€ê³  ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
        """)

    st.divider()

    st.subheader("2. ê¸°ëŒ€ íš¨ê³¼ ë° í™œìš© ë°©ì•ˆ (Expected Effects & Utilization)")
    
    with st.expander("ğŸ’¡ ìœµí•©ì  ê´€ì ì—ì„œì˜ ê¸°ëŒ€ íš¨ê³¼ (Click)", expanded=True):
        st.markdown("""
        **1) ì‹í’ˆê³µí•™ x ê²½ì œí•™ì˜ ì‹œë„ˆì§€: 'ë°ì´í„° ê¸°ë°˜ ì œí’ˆ ê¸°íš'**
        * ê¸°ì¡´ì˜ ì§ê´€ì— ì˜ì¡´í•œ ê¸°íšì—ì„œ ë²—ì–´ë‚˜, ê²€ìƒ‰ëŸ‰ ë°ì´í„°ì™€ ê²½ì œ ì§€í‘œ(ë¬¼ê°€ ìƒìŠ¹ë¥  ë“±)ë¥¼ ê²°í•©í•˜ì—¬ **'ì„±ê³µ í™•ë¥ ì´ ë†’ì€' ì œí’ˆêµ°**ì„ ì„ ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        * ì˜ˆ: ë¶ˆí™©ê¸°ì—ëŠ” 'ê°€ì„±ë¹„' í‚¤ì›Œë“œì™€ ì—°ê´€ëœ 'ëŒ€ìš©ëŸ‰/PBìƒí’ˆ' ê¸°íš, í˜¸í™©ê¸°ì—ëŠ” 'ê°€ì‹¬ë¹„' ì¤‘ì‹¬ì˜ 'í”„ë¦¬ë¯¸ì—„ ë””ì €íŠ¸' ê¸°íš ë“± ìœ ì—°í•œ ì „ëµ ìˆ˜ë¦½ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

        **2) R&D íŒŒì´í”„ë¼ì¸ ìµœì í™”**
        * êµ¬ê¸€ ìŠ¤ì¹¼ë¼ì˜ ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„ì„ í†µí•´ **í•™ê³„ì—ì„œ ì£¼ëª©ë°›ëŠ” ê¸°ìˆ **ì„ ì¡°ê¸°ì— í¬ì°©í•˜ê³ , ì´ë¥¼ ê¸°ì—…ì˜ ì„ í–‰ ì—°êµ¬ ì£¼ì œë¡œ ë¹ ë¥´ê²Œ ë„ì…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        * ì´ëŠ” ê²½ìŸì‚¬ë³´ë‹¤ í•œë°œ ì•ì„  ê¸°ìˆ  ì„ ì ê³¼ íŠ¹í—ˆ í™•ë³´ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """)

    with st.expander("ğŸš€ ì‹¤ë¬´ ë° í•™ì—… í™œìš© ì „ëµ (Strategy)"):
        st.markdown("""
        - **ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ë¦½**: ì‹œì¦ˆë„ í‚¤ì›Œë“œ(ì˜ˆ: ì—¬ë¦„ì²  'ìš”ì•„ì •', ê²¨ìš¸ì²  'í˜¸ë¹µ') ë¶„ì„ì„ í†µí•œ í”„ë¡œëª¨ì…˜ ì‹œê¸° ë° íƒ€ê²Ÿ ìµœì í™”.
        - **ê¸€ë¡œë²Œ ì§„ì¶œ ì „ëµ**: K-Food ê´€ì‹¬ë„ê°€ ë†’ì€ êµ­ê°€ì˜ í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í˜„ì§€í™” ì œí’ˆ ê°œë°œ(ì˜ˆ: ë¯¸êµ­ ì‹œì¥ ë‚´ 'ë¹„ê±´ ë§Œë‘' ìˆ˜ìš” ë¶„ì„).
        - **ìœ„ê¸° ê´€ë¦¬ ì‹œìŠ¤í…œ**: ì‹í’ˆ ì•ˆì „ ê´€ë ¨ í‚¤ì›Œë“œ(HACCP, ì‹ì¤‘ë…, ì´ë¬¼ì§ˆ)ì˜ ê²€ìƒ‰ëŸ‰ ê¸‰ì¦ì„ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§í•˜ì—¬ ì„ ì œì ì¸ í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶•.
        """)

    st.divider()

    st.subheader("3. ê³¼ì œ í›„ê¸° ë° ìê¸° ì„±ì°° (Self-Reflection)")
    st.write("ì´ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©° ëŠë‚€ ì , ê¸°ìˆ ì  ì–´ë ¤ì›€ ê·¹ë³µ ê³¼ì •, ê·¸ë¦¬ê³  ì•ìœ¼ë¡œì˜ ë‹¤ì§ì„ ììœ ë¡­ê²Œ ì‘ì„±í•©ë‹ˆë‹¤.")
    
    review = st.text_area(
        "ğŸ‘‡ ì—¬ê¸°ì— ê³¼ì œ í›„ê¸°ë¥¼ ì‘ì„±í•˜ì„¸ìš” (ì‘ì„± í›„ Ctrl+Enterë¥¼ ëˆ„ë¥´ë©´ ì €ì¥ë©ë‹ˆë‹¤)",
        height=150,
        placeholder="ì˜ˆì‹œ: ì²˜ìŒì—ëŠ” íŒŒì´ì¬ ì½”ë“œê°€ ë‚¯ì„¤ì—ˆì§€ë§Œ, ì§ì ‘ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ê³  ì‹œê°í™”í•´ë³´ë‹ˆ ë°ì´í„°ì˜ í˜ì„ ì‹¤ê°í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ê²½ì œí•™ ìˆ˜ì—…ì—ì„œ ë°°ìš´ ìˆ˜ìš” ì˜ˆì¸¡ ì´ë¡ ì„ ì‹¤ì œ ê²€ìƒ‰ëŸ‰ ë°ì´í„°ì— ì ìš©í•´ë³´ê³  ì‹¶ì€ ìš•ì‹¬ì´ ìƒê²¼ìŠµë‹ˆë‹¤. Selenium í¬ë¡¤ë§ ê³¼ì •ì—ì„œ ë´‡ íƒì§€ ë¬¸ì œë¥¼ í•´ê²°í•˜ë©° ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ë„ ê¸°ë¥¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤."
    )
    
    if review:
        st.success("âœ… í›„ê¸°ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤! (ì´ ë‚´ìš©ì€ ë°œí‘œ ì‹œ í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤)")
        st.write(f"**ì‘ì„±ëœ ë‚´ìš©:** {review}")

# =========================================================
# ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# =========================================================

def main():
    st.sidebar.title("ğŸ—‚ï¸ Portfolio Menu")
    
    menu = st.sidebar.radio(
        "ì´ë™í•  í˜ì´ì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”",
        [
            "1. Intro: ìœµí•© ì¸ì¬ í¬íŠ¸í´ë¦¬ì˜¤", 
            "2. Trend: í‚¤ì›Œë“œ ê²€ìƒ‰ ìˆ˜ ë¶„ì„", 
            "3. Map: ì‹í’ˆ ê¸°ì—… ìˆœìœ„ ë° ì§€ë„", 
            "4. Info: 10ëŒ€ ê¸°ì—… ìƒì„¸ ì •ë³´", 
            "5. Research: ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„", 
            "6. Conclusion: ê²°ë¡  ë° ì œì–¸" 
        ]
    )

    if menu.startswith("1."):
        page_intro()
    elif menu.startswith("2."):
        page_keyword_analysis()
    elif menu.startswith("3."):
        page_map_visualization()
    elif menu.startswith("4."):
        page_company_info()
    elif menu.startswith("5."):
        page_scholar_analysis()
    elif menu.startswith("6."):
        page_conclusion()

if __name__ == "__main__":
    main()




